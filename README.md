# a-python-crawler-for-58.com
1.climbing second-hand goods information from http://bj.58.com/sale.shtml.
2.The channel_extract file is used to get url_links under "类目导航".
3.The Page_parsing file is used to get url_links and informations of commodities excepet "转转商品".
4.The main file is the main program, which is used to call other files.
5.The count file is used to count numbers of commodities url_links.
6.This project used MongoDB as the repository.
